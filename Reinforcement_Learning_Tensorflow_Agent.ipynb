{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlGF-xcyawPW"
   },
   "source": [
    "# Deep Reinforcement learning with TF-Agents\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_I-j7kjbYoh"
   },
   "source": [
    "The goal of this project is to implement reinforcement agents with following setps:\n",
    "\n",
    "* Choose one environment from [OpenAI Gym](https://gym.openai.com/envs/#classic_control)\n",
    "\n",
    "* Choose one agent from [tf.agents library](https://github.com/tensorflow/agents)\n",
    "\n",
    "* Study and explain the policy learning method for the particular agent of your choice\n",
    "\n",
    "* Use the training procedure in the [tutorial](https://github.com/lydiahsu/AML_Fall_2019/blob/master/TF_agents_intro.ipynb) as a reference and implement your own agent\n",
    "\n",
    "* Show the performance of your agent by letting the trained agent to play several random games and compare the total rewards with the maximum possible rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yQNbrnm2YBwv"
   },
   "source": [
    "We have modified the goal to implement both Double Q-Network and Soft Actor Critic on three classic games with discrete action spaces. Since SAC doesn't already have an implementation in the discrete setting, we have written that ourselves \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gsHHW9DlWnJ2"
   },
   "source": [
    "## Choosing agents from `tf.library`\n",
    "\n",
    "We chose __Deep Q Network (DQN)__ and __Soft Actor Critic(SAC)__ agents. In the following sections, we will implement the agents through `tf_agent` and run them on the `MountainCar-v0`, `Acrobot-v1` and `CartPole-v0` environments. Towards the end, we also tried SAC agent for the Atari Game: Boxing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NZku1_JjK_3a"
   },
   "source": [
    "### Setup \n",
    "\n",
    "install libraries with correct version of TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "w7s-IzEhW01s",
    "outputId": "0e99a7ca-a79b-4c46-f4e4-989a8ff1a578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PILLOW in /usr/local/lib/python3.6/dist-packages (4.3.0)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from PILLOW) (0.46)\n",
      "Collecting tf-agents-nightly\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/e1/e65c1603835f3967da1ca7f1459981e39f2dbf3e3723a7cd567aa77de731/tf_agents_nightly-0.2.0.dev20191215-py2.py3-none-any.whl (848kB)\n",
      "\u001b[K     |████████████████████████████████| 849kB 2.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents-nightly) (1.12.0)\n",
      "Collecting gin-config==0.1.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/be/c984b1c8a7ba1c385b32bf39c7a225cd9f713d49705898309d01b60fd0e7/gin_config-0.1.3-py3-none-any.whl (43kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 9.7MB/s \n",
      "\u001b[?25hCollecting tfp-nightly\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a4/4cca762fcda43a8c89f0886dfd516a300c0027d084360315ac5525569038/tfp_nightly-0.9.0.dev20191215-py2.py3-none-any.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 51.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents-nightly) (1.17.4)\n",
      "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-agents-nightly) (0.8.1)\n",
      "Requirement already satisfied: gast>=0.2 in /usr/local/lib/python3.6/dist-packages (from tfp-nightly->tf-agents-nightly) (0.2.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.2 in /usr/local/lib/python3.6/dist-packages (from tfp-nightly->tf-agents-nightly) (1.2.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tfp-nightly->tf-agents-nightly) (4.4.1)\n",
      "Installing collected packages: gin-config, tfp-nightly, tf-agents-nightly\n",
      "  Found existing installation: gin-config 0.2.1\n",
      "    Uninstalling gin-config-0.2.1:\n",
      "      Successfully uninstalled gin-config-0.2.1\n",
      "Successfully installed gin-config-0.1.3 tf-agents-nightly-0.2.0.dev20191215 tfp-nightly-0.9.0.dev20191215\n",
      "TensorFlow 2.x selected.\n"
     ]
    }
   ],
   "source": [
    "!pip install PILLOW\n",
    "!pip install tf-agents-nightly\n",
    "try:\n",
    "  %%tensorflow_version 2.x\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QezVTjuJMMQ4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver, dynamic_episode_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics, py_metric, tf_py_metric\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LNE0dSOEPilX"
   },
   "source": [
    "### Setup Environment\n",
    "We tried DQN on three environments, MountainCar-v0, Acrobot-v1 and CartPole-v0. To run the code for the others, just update the env_name variable here. For CartPole, the hyperparameters also require a small change. Please be mindful of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NnCloCRaPnY5"
   },
   "outputs": [],
   "source": [
    "env_name = 'MountainCar-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "wkd2V9AaDLav",
    "outputId": "091b2cfa-c09d-4bef-8aff-08759eab4046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(2,), dtype=dtype('float32'), name='observation', minimum=[-1.2  -0.07], maximum=[0.6  0.07])\n",
      "Reward Spec:\n",
      "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=2)\n"
     ]
    }
   ],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(train_py_env.time_step_spec().observation)\n",
    "print('Reward Spec:')\n",
    "print(train_py_env.time_step_spec().reward)\n",
    "print('Action Spec:')\n",
    "print(train_py_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKhjp80GP_I9"
   },
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QdInGznwKs0u"
   },
   "source": [
    "## Run DQN First"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eO86cuJ9XtEW"
   },
   "source": [
    "### Hyperparameters\n",
    "\n",
    "#### The default parameters are set for MountainCar and Acrobot. For CartPole, we changed the batch_size to 64 and learning_rate to 1e-3. Everything else remains unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eswWQ7VQXyOJ"
   },
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size =   128# @param {type:\"integer\"}\n",
    "learning_rate = 1e-4  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5Wi7IaKqSL5"
   },
   "source": [
    "### Agent\n",
    "\n",
    "The algorithm used is __DQN__. The __DQN__ agent can be used in any environment which has a discrete action space. At the heart of a __DQN__ Agent is a `QNetwork`, a neural network model that can learn to predict `QValues` (expected returns) for all actions, given an observation from the environment.\n",
    "\n",
    "Use `tf_agents.networks.q_network` to create a `QNetwork`, passing in the `observation_spec`, `action_spec`, and a tuple describing the number and size of the model's hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UjnBSSpzsdO-"
   },
   "outputs": [],
   "source": [
    "fc_layer_params = (100,)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPPvDTc2xQ8p"
   },
   "source": [
    "Now use `tf_agents.dqn.dqn_agent` to instantiate a `DqnAgent`. In addition to the `time_step_spec`, `action_spec` and the QNetwork, the agent constructor also requires an optimizer (in this case, `AdamOptimizer`), a loss function, and an integer step counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NgUXHrYtygl7"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "dqn_agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter\n",
    ")\n",
    "\n",
    "dqn_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9nLeElAJ8Qon"
   },
   "source": [
    "### Replay Buffer\n",
    "\n",
    "The replay buffer keeps track of data collected from the environment. This tutorial uses `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer`, as it is the most common. \n",
    "\n",
    "The constructor requires the specs for the data it will be collecting. This is available from the agent using the `collect_data_spec` method. The batch size and maximum buffer length are also required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ZBWss248SrB"
   },
   "outputs": [],
   "source": [
    "dqn_replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=dqn_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kgyPU9_K8gwS"
   },
   "source": [
    "### Data Collection with customized metric\n",
    "\n",
    "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hRAY6XSrMoec"
   },
   "outputs": [],
   "source": [
    "class MaxEpisodeScoreMetric(py_metric.PyStepMetric):\n",
    "  def __init__(self, name='MaxEpisodeScoreMetric'):\n",
    "    super(py_metric.PyStepMetric, self).__init__(name)\n",
    "    self.rewards = []\n",
    "    self.discounts = []\n",
    "    self.max_discounted_reward = None\n",
    "    self.reset()\n",
    "  def reset(self):\n",
    "    self.rewards = []\n",
    "    self.discounts = []\n",
    "    self.max_discounted_reward = None\n",
    "  def call(self, trajectory):\n",
    "    self.rewards += trajectory.reward\n",
    "    self.discounts += trajectory.discount\n",
    "    \n",
    "    if(trajectory.is_last()):      \n",
    "      adjusted_discounts = [1.0] + self.discounts # because a step has its value + the discount of the NEXT step (Bellman equation)\n",
    "      adjusted_discounts = adjusted_discounts[:-1] # dropping the discount of the last step because it is not followed by a next step, so the value is useless\n",
    "      discounted_reward = np.sum(np.multiply(self.rewards, adjusted_discounts))\n",
    "      #print(self.rewards, adjusted_discounts, discounted_reward)\n",
    "      \n",
    "      if self.max_discounted_reward == None:\n",
    "        self.max_discounted_reward = discounted_reward\n",
    "      \n",
    "      if discounted_reward > self.max_discounted_reward:\n",
    "        self.max_discounted_reward = discounted_reward\n",
    "        \n",
    "      self.rewards = []\n",
    "      self.discounts = []\n",
    "  def result(self):\n",
    "    return self.max_discounted_reward\n",
    "\n",
    "class TFMaxEpisodeScoreMetric(tf_py_metric.TFPyMetric):\n",
    "\n",
    "  def __init__(self, name='MaxEpisodeScoreMetric', dtype=tf.float32):\n",
    "    py_metric = MaxEpisodeScoreMetric()\n",
    "\n",
    "    super(TFMaxEpisodeScoreMetric, self).__init__(\n",
    "        py_metric=py_metric, name=name, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kkRGzPRt8rjQ"
   },
   "outputs": [],
   "source": [
    "def collect_training_data(agent, replay_buffer):\n",
    "  dynamic_step_driver.DynamicStepDriver(\n",
    "      train_env,\n",
    "      agent.collect_policy,\n",
    "      observers=[replay_buffer.add_batch],\n",
    "      num_steps=initial_collect_steps).run()\n",
    "\n",
    "def train_agent(agent, replay_buffer):\n",
    "  dataset = replay_buffer.as_dataset(\n",
    "      sample_batch_size=batch_size,\n",
    "      num_steps=2)\n",
    "\n",
    "  iterator = iter(dataset)\n",
    "\n",
    "  loss = None\n",
    "  for _ in tqdm(range(num_iterations)):\n",
    "    trajectories, _ = next(iterator)\n",
    "    loss = agent.train(experience=trajectories)\n",
    "  \n",
    "  print('\\nTraining loss: ', loss.loss.numpy())\n",
    "  return loss.loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9DM0wHL3XWpF"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(agent):\n",
    "  max_score = TFMaxEpisodeScoreMetric() \n",
    "  observers = [max_score]\n",
    "  driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "      train_env, agent.policy, observers, num_episodes=num_eval_episodes)\n",
    "\n",
    "  final_time_step, policy_state = driver.run()\n",
    "\n",
    "  print('Max test score:', max_score.result().numpy())\n",
    "  return max_score.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHeZ7O5k-PuI"
   },
   "source": [
    "### Training the agent\n",
    "\n",
    "Two thing must happen during the training loop:\n",
    "\n",
    "-   collect data from the environment\n",
    "-   use that data to train the agent's neural network(s)\n",
    "\n",
    "This example also periodicially evaluates the policy and prints the current score.\n",
    "\n",
    "The following will take ~5 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xyilPNyTAQ-d",
    "outputId": "39a64e56-ff14-4698-ff92-dc36b04a5ccb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "kvZjyvrkXqWT",
    "outputId": "f5be298a-0cde-4c69-b548-c57e7c2fe023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [08:20<00:00, 39.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  7.656824\n",
      "Max test score: -200.0\n",
      "Step  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [08:13<00:00, 40.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  15.141324\n",
      "Max test score: -200.0\n",
      "Step  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [08:16<00:00, 40.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  25.84581\n",
      "Max test score: -200.0\n",
      "Step  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [08:22<00:00, 39.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  25.196678\n",
      "Max test score: -200.0\n",
      "Step  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [08:21<00:00, 42.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  28.91012\n",
      "Max test score: -200.0\n",
      "Step  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [08:19<00:00, 40.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  24.963537\n",
      "Max test score: -200.0\n",
      "Step  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [08:21<00:00, 39.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  17.815933\n",
      "Max test score: -200.0\n",
      "Step  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [08:20<00:00, 39.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  29.956268\n",
      "Max test score: -200.0\n",
      "Step  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [08:27<00:00, 39.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  57.86882\n",
      "Max test score: -200.0\n",
      "Step  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [08:19<00:00, 40.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  1048.5605\n",
      "Max test score: -200.0\n"
     ]
    }
   ],
   "source": [
    "dqn_training_loss = []\n",
    "dqn_max_test_score = []\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "  for i in range(10):\n",
    "    print('Step ', i+1)\n",
    "    collect_training_data(dqn_agent, dqn_replay_buffer)\n",
    "    dqn_training_loss.append(train_agent(dqn_agent, dqn_replay_buffer))\n",
    "    dqn_max_test_score.append(evaluate_agent(dqn_agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "24JAe3VjNCG1",
    "outputId": "dfc131ca-e444-4bcd-8afa-94950b9361c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7a2676d550>"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEWCAYAAACOv5f1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(1, 11, step = 1), dqn_training_loss, c = 'orange', label = 'Training loss')\n",
    "plt.plot(np.arange(1, 11, step = 1), dqn_max_test_score, c = 'blue', label = 'Max test score')\n",
    "#plt.axhline(1.715, c = 'gray', linestyle='dashed', label = 'Max possible score')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)\n",
    "plt.title('Training loss and max test score')\n",
    "plt.xticks(np.arange(1, 11))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "38gFz3JrZ_Ih"
   },
   "source": [
    "Exporting results to a PKL file so that we can plot them together later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qu_WVELF3Gnp"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('DQN-Loss-MountainCar.pkl', 'wb') as f:\n",
    "  pickle.dump(dqn_training_loss, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T7fDhviJrhz4"
   },
   "source": [
    "## *SAC Agent*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ywsi3RCkWkDM"
   },
   "source": [
    "## Implementation of SAC for discrete action space\n",
    "\n",
    "* Below is an implementation of SAC in discrete action space in `tf.agent` according to the algorithm proposed in [P. Christodoulou 2019](https://arxiv.org/abs/1910.07207)\n",
    "\n",
    "* The code is inspired by the pytorch implementation of the above algorithm found [here](https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/blob/master/agents/actor_critic_agents/SAC_Discrete.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c941UnWedpEK"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.policies import actor_policy\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.utils import eager_utils\n",
    "from tf_agents.utils import nest_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exUP_TluaUWN"
   },
   "source": [
    "Writing policy and computing losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZkPG-UFcXfSu"
   },
   "outputs": [],
   "source": [
    "SacLossInfo = collections.namedtuple(\n",
    "    'SacLossInfo', ('critic_loss', 'actor_loss', 'alpha_loss'))\n",
    "\n",
    "class SacDisAgent(tf_agent.TFAgent):\n",
    "\n",
    "  def __init__(self,\n",
    "               time_step_spec,\n",
    "               action_spec,\n",
    "               critic_network,\n",
    "               actor_network,\n",
    "               actor_optimizer,\n",
    "               critic_optimizer,\n",
    "               alpha_optimizer,\n",
    "               target_critic_network=None,\n",
    "               target_critic_network_2=None,\n",
    "               actor_policy_ctor=actor_policy.ActorPolicy,\n",
    "               target_update_tau=1.0,\n",
    "               target_update_period=1,\n",
    "               td_errors_loss_fn=tf.math.squared_difference,\n",
    "               gamma=1.0,\n",
    "               reward_scale_factor=1.0,\n",
    "               initial_log_alpha=0.0,\n",
    "               gradient_clipping=None,\n",
    "               debug_summaries=False,\n",
    "               summarize_grads_and_vars=False,\n",
    "               train_step_counter=None,\n",
    "               name=None):\n",
    "    tf.Module.__init__(self, name=name)\n",
    "\n",
    "    flat_action_spec = tf.nest.flatten(action_spec)\n",
    "    for spec in flat_action_spec:\n",
    "      if not spec.dtype.is_integer:\n",
    "        raise TypeError(\n",
    "            'SacDisAgent is only for discrete action space. Use SacAgent for continuous'\n",
    "        )\n",
    "\n",
    "    self._critic_network_1 = critic_network\n",
    "    self._critic_network_1.create_variables()\n",
    "    self._target_critic_network_1 = (\n",
    "        common.maybe_copy_target_network_with_checks(self._critic_network_1,\n",
    "                                                     target_critic_network,\n",
    "                                                     'TargetCriticNetwork1'))\n",
    "    self._critic_network_2 = critic_network.copy(name=\"CriticNetwork2\")\n",
    "    self._critic_network_2.create_variables()\n",
    "    self._target_critic_network_2 = (\n",
    "        common.maybe_copy_target_network_with_checks(self._critic_network_2,\n",
    "                                                     target_critic_network_2,\n",
    "                                                     'TargetCriticNetwork2'))\n",
    "\n",
    "    self._actor_network = actor_network\n",
    "    self._actor_network.create_variables()\n",
    "\n",
    "    policy = actor_policy_ctor(\n",
    "        time_step_spec = time_step_spec,\n",
    "        action_spec = action_spec,\n",
    "        actor_network = self._actor_network,\n",
    "        training=False\n",
    "    )\n",
    "\n",
    "    self._train_policy = actor_policy_ctor(\n",
    "        time_step_spec = time_step_spec,\n",
    "        action_spec = action_spec,\n",
    "        actor_network = self._actor_network,\n",
    "        training=True\n",
    "    )\n",
    "\n",
    "    self._log_alpha = common.create_variable(\n",
    "        'initial_log_alpha',\n",
    "        initial_value = initial_log_alpha,\n",
    "        dtype = tf.float32,\n",
    "        trainable=True\n",
    "    )\n",
    "\n",
    "    flat_action_spec = tf.nest.flatten(action_spec)\n",
    "    target_entropy = -np.sum([\n",
    "                              np.product(single_spec.shape.as_list())\n",
    "                              for single_spec in flat_action_spec])\n",
    "\n",
    "    self._target_update_tau = target_update_tau\n",
    "    self._target_update_period = target_update_period\n",
    "    self._actor_optimizer = actor_optimizer\n",
    "    self._critic_optimizer = critic_optimizer\n",
    "    self._alpha_optimizer = alpha_optimizer\n",
    "    self._td_errors_loss_fn = td_errors_loss_fn\n",
    "    self._gamma = gamma\n",
    "    self._reward_scale_factor = reward_scale_factor\n",
    "    self._target_entropy = target_entropy\n",
    "    self._gradient_clipping = gradient_clipping\n",
    "    self._debug_summaries = debug_summaries\n",
    "    self._summarize_grads_and_vars = summarize_grads_and_vars\n",
    "    self._update_target = self._get_target_updater(tau=self._target_update_tau, period=self._target_update_period)\n",
    "\n",
    "    train_sequence_length = 2 if not critic_network.state_spec else None\n",
    "\n",
    "    super(SacDisAgent, self).__init__(\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        policy=policy,\n",
    "        collect_policy=policy,\n",
    "        train_sequence_length=train_sequence_length,\n",
    "        debug_summaries=debug_summaries,\n",
    "        summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "        train_step_counter=train_step_counter)\n",
    "    \n",
    "  def _initialize(self):\n",
    "    \"Operation that initialize the agent which copies all the parameters into the Q-network\"\n",
    "    common.soft_variables_update(\n",
    "        self._critic_network_1.variables,\n",
    "        self._target_critic_network_1.variables,\n",
    "        tau=1.0)\n",
    "    common.soft_variables_update(\n",
    "        self._critic_network_2.variables,\n",
    "        self._target_critic_network_2.variables,\n",
    "        tau=1.0)\n",
    "    \n",
    "  def _experience_to_transitions(self, experience):\n",
    "    transitions = trajectory.to_transition(experience)\n",
    "    time_steps, policy_steps, next_time_steps = transitions\n",
    "    actions = policy_steps.action\n",
    "    if (self.train_sequence_length is not None and\n",
    "        self.train_sequence_length == 2):\n",
    "      # Sequence empty time dimension if critic network is stateless.\n",
    "      time_steps, actions, next_time_steps = tf.nest.map_structure(\n",
    "          lambda t: tf.squeeze(t, axis=1),\n",
    "          (time_steps, actions, next_time_steps))\n",
    "    return time_steps, actions, next_time_steps\n",
    "    \n",
    "  def _apply_gradients(self, gradients, variables, optimizer):\n",
    "    grads_and_vars = list(zip(gradients, variables))\n",
    "    if self._gradient_clipping is not None:\n",
    "      grads_and_vars = eager_utils.clip_gradient_norms(grads_and_vars,\n",
    "                                                        self._gradient_clipping)\n",
    "    if self._summarize_grads_and_vars:\n",
    "      eager_utils.add_variables_summaries(grads_and_vars,\n",
    "                                        self.train_step_counter)\n",
    "      eager_utils.add_gradients_summaries(grads_and_vars,\n",
    "                                        self.train_step_counter)\n",
    "    optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "  def _get_target_updater(self, tau=1.0, period=1):\n",
    "    \"\"\"Performs a soft update of the target network parameters.\n",
    "    For each weight w_s in the original network, and its corresponding\n",
    "    weight w_t in the target network, a soft update is:\n",
    "    w_t = (1- tau) x w_t + tau x ws\n",
    "    Args:\n",
    "      tau: A float scalar in [0, 1]. Default `tau=1.0` means hard update.\n",
    "      period: Step interval at which the target network is updated.\n",
    "    Returns:\n",
    "      A callable that performs a soft update of the target network parameters.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('update_target'):\n",
    "\n",
    "      def update():\n",
    "        \"\"\"Update target network.\"\"\"\n",
    "        critic_update_1 = common.soft_variables_update(\n",
    "            self._critic_network_1.variables,\n",
    "            self._target_critic_network_1.variables,\n",
    "            tau,\n",
    "            tau_non_trainable=1.0)\n",
    "        critic_update_2 = common.soft_variables_update(\n",
    "            self._critic_network_2.variables,\n",
    "            self._target_critic_network_2.variables,\n",
    "            tau,\n",
    "            tau_non_trainable=1.0)\n",
    "        return tf.group(critic_update_1, critic_update_2)\n",
    "      return common.Periodically(update, period, 'update_targets')\n",
    "\n",
    "    \n",
    "  def _actions_and_log_probs(self, time_steps):\n",
    "    \"get actions and corresponding probabilities from policy.\"\n",
    "    \n",
    "    batch_size = nest_utils.get_outer_shape(time_steps, self.time_step_spec)[0]\n",
    "    policy_state = self._train_policy.get_initial_state(batch_size)\n",
    "    action_distribution = self._train_policy.distribution(\n",
    "        time_steps, policy_state = policy_state\n",
    "    ).action\n",
    "\n",
    "    actions = tf.nest.map_structure(lambda d: d.sample(), action_distribution)\n",
    "    log_prob = common.log_probability(action_distribution, actions, self.action_spec)\n",
    "    prob = tf.exp(log_prob)\n",
    "\n",
    "    #tf.print(\"Probabilities: \", log_prob, prob, sep=\">>>>\\n\")\n",
    "\n",
    "    return actions, log_prob, prob\n",
    "\n",
    "  def critic_loss(self,\n",
    "                  time_steps,\n",
    "                  actions,\n",
    "                  next_time_steps,\n",
    "                  td_errors_loss_fn,\n",
    "                  gamma = 1.0,\n",
    "                  reward_scale_factor = 1.0,\n",
    "                  weights = None):\n",
    "    \"Computes the critic loss for training\"\n",
    "\n",
    "    with tf.name_scope('critic_loss'):\n",
    "      tf.nest.assert_same_structure(actions, self.action_spec)\n",
    "      tf.nest.assert_same_structure(time_steps, self.time_step_spec)\n",
    "      tf.nest.assert_same_structure(next_time_steps, self.time_step_spec)\n",
    "\n",
    "      next_actions, next_log_prob, next_prob = self._actions_and_log_probs(next_time_steps)\n",
    "      target_input = (next_time_steps.observation, next_actions)\n",
    "      target_q_values1, _ = self._target_critic_network_1(\n",
    "          target_input, next_time_steps.step_type, training=False)\n",
    "      target_q_values2, _ = self._target_critic_network_2(\n",
    "          target_input, next_time_steps.step_type, training=False)\n",
    "      target_q_values = (\n",
    "          tf.minimum(target_q_values1, target_q_values2) -\n",
    "          tf.exp(self._log_alpha) * next_log_prob)\n",
    "      target_q_values = next_prob * target_q_values\n",
    "\n",
    "      td_targets = tf.stop_gradient(\n",
    "          reward_scale_factor * next_time_steps.reward +\n",
    "          gamma * next_time_steps.discount * target_q_values)\n",
    "\n",
    "      pred_input = (time_steps.observation, actions)\n",
    "      pred_td_targets1, _ = self._critic_network_1(\n",
    "          pred_input, time_steps.step_type, training=True)\n",
    "      pred_td_targets2, _ = self._critic_network_2(\n",
    "          pred_input, time_steps.step_type, training=True)\n",
    "      critic_loss1 = td_errors_loss_fn(td_targets, pred_td_targets1)\n",
    "      critic_loss2 = td_errors_loss_fn(td_targets, pred_td_targets2)\n",
    "      critic_loss = critic_loss1 + critic_loss2\n",
    "\n",
    "      if weights is not None:\n",
    "        critic_loss *= weights\n",
    "\n",
    "      if nest_utils.is_batched_nested_tensors(time_steps, self.time_step_spec, num_outer_dims=2):\n",
    "        # Sum over the time dimension.\n",
    "        critic_loss = tf.reduce_sum(input_tensor=critic_loss, axis=1)\n",
    "      # Take the mean across the batch.\n",
    "      critic_loss = tf.reduce_mean(input_tensor=critic_loss)\n",
    "\n",
    "      if self._debug_summaries:\n",
    "        td_errors1 = td_targets - pred_td_targets1\n",
    "        td_errors2 = td_targets - pred_td_targets2\n",
    "        td_errors = tf.concat([td_errors1, td_errors2], axis=0)\n",
    "        common.generate_tensor_summaries('td_errors', td_errors,\n",
    "                                         self.train_step_counter)\n",
    "        common.generate_tensor_summaries('td_targets', td_targets,\n",
    "                                         self.train_step_counter)\n",
    "        common.generate_tensor_summaries('pred_td_targets1', pred_td_targets1,\n",
    "                                         self.train_step_counter)\n",
    "        common.generate_tensor_summaries('pred_td_targets2', pred_td_targets2,\n",
    "                                         self.train_step_counter)\n",
    "\n",
    "\n",
    "      return critic_loss\n",
    "\n",
    "  def actor_loss(self, time_steps, weights=None):\n",
    "    \"Computes the actor loss for training\"\n",
    "    with tf.name_scope('actor_loss'):\n",
    "      tf.nest.assert_same_structure(time_steps, self.time_step_spec)\n",
    "\n",
    "      actions, log_prob, prob = self._actions_and_log_probs(time_steps)\n",
    "      target_input = (time_steps.observation, actions)\n",
    "      target_q_values1, _ = self._critic_network_1(target_input, time_steps.step_type, training=False)\n",
    "      target_q_values2, _ = self._critic_network_2(target_input, time_steps.step_type, training=False)\n",
    "      target_q_values = tf.minimum(target_q_values1, target_q_values2)\n",
    "      actor_loss = tf.exp(self._log_alpha) * log_prob - target_q_values\n",
    "      actor_loss = actor_loss * prob\n",
    "      if nest_utils.is_batched_nested_tensors(time_steps, self.time_step_spec, num_outer_dims=2):\n",
    "        actor_loss = tf.reduce_sum(input_tensor=actor_loss, axis=1)\n",
    "      if weights is not None:\n",
    "        actor_loss *= weights\n",
    "      actor_loss = tf.reduce_mean(input_tensor=actor_loss)\n",
    "\n",
    "\n",
    "      if self._debug_summaries:\n",
    "        common.generate_tensor_summaries('actor_loss', actor_loss,\n",
    "                                         self.train_step_counter)\n",
    "        common.generate_tensor_summaries('actions', actions,\n",
    "                                         self.train_step_counter)\n",
    "        common.generate_tensor_summaries('log_pi', log_prob,\n",
    "                                         self.train_step_counter)\n",
    "        tf.compat.v2.summary.scalar(\n",
    "            name='entropy_avg',\n",
    "            data=-tf.reduce_mean(input_tensor=log_prob),\n",
    "            step=self.train_step_counter)\n",
    "        common.generate_tensor_summaries('target_q_values', target_q_values,\n",
    "                                         self.train_step_counter)\n",
    "        batch_size = nest_utils.get_outer_shape(\n",
    "            time_steps, self._time_step_spec)[0]\n",
    "        policy_state = self._train_policy.get_initial_state(batch_size)\n",
    "        action_distribution = self._train_policy.distribution(\n",
    "            time_steps, policy_state).action\n",
    "        if isinstance(action_distribution, tfp.distributions.Normal):\n",
    "          common.generate_tensor_summaries('act_mean', action_distribution.loc,\n",
    "                                           self.train_step_counter)\n",
    "          common.generate_tensor_summaries(\n",
    "              'act_stddev', action_distribution.scale, self.train_step_counter)\n",
    "        elif isinstance(action_distribution, tfp.distributions.Categorical):\n",
    "          common.generate_tensor_summaries(\n",
    "              'act_mode', action_distribution.mode(), self.train_step_counter)\n",
    "        try:\n",
    "          common.generate_tensor_summaries('entropy_action',\n",
    "                                           action_distribution.entropy(),\n",
    "                                           self.train_step_counter)\n",
    "        except NotImplementedError:\n",
    "          pass  # Some distributions do not have an analytic entropy.\n",
    "\n",
    "      return actor_loss\n",
    "\n",
    "  def alpha_loss(self, time_steps, weights=None):\n",
    "    with tf.name_scope('alpha_loss'):\n",
    "      tf.nest.assert_same_structure(time_steps, self.time_step_spec)\n",
    "\n",
    "      actions, log_prob, log_pi = self._actions_and_log_probs(time_steps)\n",
    "      log_pi = tf.reduce_sum(input_tensor = log_prob * prob)\n",
    "      entropy_diff = tf.stop_gradient(-log_pi - self._target_entropy)\n",
    "      alpha_loss = (self._log_alpha * entropy_diff)\n",
    "      if nest_utils.is_batched_nested_tensors(time_steps, self.time_step_spec, num_outer_dims=2):\n",
    "        alpha_loss = tf.reduce_sum(input_tensor=alpha_loss, axis=1)\n",
    "      if weights is not None:\n",
    "        alpha_loss *= weights\n",
    "\n",
    "      alpha_loss = tf.reduce_mean(input_tensor=alpha_loss)\n",
    "      return alpha_loss\n",
    "\n",
    "  def _train(self, experience, weights):\n",
    "    \"trains with the provided batched experience\"\n",
    "\n",
    "    time_steps, actions, next_time_steps = self._experience_to_transitions(experience)\n",
    "\n",
    "    trainable_critic_variables = (\n",
    "        self._critic_network_1.trainable_variables +\n",
    "        self._critic_network_2.trainable_variables\n",
    "    )\n",
    "\n",
    "    \n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "      assert trainable_critic_variables, ('No trainable critic variables to optimize.')\n",
    "\n",
    "      tape.watch(trainable_critic_variables)\n",
    "      critic_loss = self.critic_loss(\n",
    "          time_steps,\n",
    "          actions,\n",
    "          next_time_steps,\n",
    "          td_errors_loss_fn = self._td_errors_loss_fn,\n",
    "          gamma = self._gamma,\n",
    "          reward_scale_factor = self._reward_scale_factor,\n",
    "          weights = weights\n",
    "      )\n",
    "    tf.debugging.check_numerics(critic_loss, 'Critic loss is inf or nan.')\n",
    "    critic_grads = tape.gradient(critic_loss, trainable_critic_variables)\n",
    "    self._apply_gradients(critic_grads, trainable_critic_variables, self._critic_optimizer)\n",
    "\n",
    "    trainable_actor_variables = self._actor_network.trainable_variables\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "      assert trainable_actor_variables, ('No trainable actor variables to optimize.')\n",
    "\n",
    "      tape.watch(trainable_actor_variables)\n",
    "      actor_loss = self.actor_loss(time_steps, weights=weights)\n",
    "    tf.debugging.check_numerics(actor_loss, 'Actor loss is inf or nan.')\n",
    "    actor_grads = tape.gradient(actor_loss, trainable_actor_variables)\n",
    "    self._apply_gradients(actor_grads, trainable_actor_variables, self._actor_optimizer)\n",
    "\n",
    "    alpha_variable = [self._log_alpha]\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "      assert alpha_variable, (\"No alpha variable to optimize.\")\n",
    "      tape.watch(alpha_variable)\n",
    "      alpha_loss = self.alpha_loss(time_steps, weights=weights)\n",
    "    tf.debugging.check_numerics(alpha_loss, 'Alpha lost is inf or nan.')\n",
    "    alpha_grads = tape.gradient(alpha_loss, alpha_variable)\n",
    "    self._apply_gradients(alpha_grads, alpha_variable, self._alpha_optimizer)\n",
    "\n",
    "    with tf.name_scope(\"Losses\"):\n",
    "      tf.compat.v2.summary.scalar(name='critic_loss', data=critic_loss, step=self.train_step_counter)\n",
    "      tf.compat.v2.summary.scalar(name='actor_loss', data=actor_loss, step=self.train_step_counter)\n",
    "      tf.compat.v2.summary.scalar(name='alpha_loss', data=alpha_loss, step=self.train_step_counter)\n",
    "\n",
    "    self.train_step_counter.assign_add(1)\n",
    "    self._update_target()\n",
    "\n",
    "    total_loss = critic_loss + actor_loss + alpha_loss\n",
    "\n",
    "    extra = SacLossInfo(\n",
    "        critic_loss = critic_loss,\n",
    "        actor_loss = actor_loss,\n",
    "        alpha_loss = alpha_loss\n",
    "    )\n",
    "\n",
    "    return tf_agent.LossInfo(loss = total_loss, extra=extra)\n",
    "\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TQFVmFDB4ppK"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "`# This is formatted as code`\n",
    "```\n",
    "\n",
    "### Start to apply the SAC agent in the chosen environments\n",
    "\n",
    "Trying to compare the performance of SAC discrete agent against the DQN. Start with importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO3ZPb9l5FtM"
   },
   "outputs": [],
   "source": [
    "from tf_agents.drivers import dynamic_step_driver, dynamic_episode_driver\n",
    "from tf_agents.environments import suite_gym, tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.metrics import tf_metrics, py_metric, tf_py_metric\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-WQCLIok6Gzb"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UiWEbrcmb-gr"
   },
   "source": [
    "To run the other environments, please update the env_name to Acrobot-v1 or MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RI5AkdnU59ic"
   },
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v0\" # @param {type:\"string\"}\n",
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000 # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 1000000 # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 256 # @param {type:\"integer\"}\n",
    "\n",
    "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "target_update_tau = 0.005 # @param {type:\"number\"}\n",
    "target_update_period = 1 # @param {type:\"number\"}\n",
    "gamma = 0.99 # @param {type:\"number\"}\n",
    "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
    "gradient_clipping = None # @param\n",
    "\n",
    "actor_fc_layer_params = (256, 256)\n",
    "critic_joint_fc_layer_params = (256, 256)\n",
    "\n",
    "log_interval = 5000 # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 30 # @param {type:\"integer\"}\n",
    "eval_interval = 1000 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XrKPy8NatgYS"
   },
   "source": [
    "##### We will implement the same hyperparameters for Acrobot Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XrmxP4vU6QDd"
   },
   "source": [
    "### Import the environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "ewLeCHzr6eCU",
    "outputId": "c6a14d40-9657-43c5-ae80-ebfa8a72252f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(2,), dtype=dtype('float32'), name='observation', minimum=[-1.2  -0.07], maximum=[0.6  0.07])\n",
      "Reward Spec:\n",
      "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
      "Action Spec:\n",
      "BoundedArraySpec(shape=(1,), dtype=dtype('float32'), name='action', minimum=-1.0, maximum=1.0)\n"
     ]
    }
   ],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(train_py_env.time_step_spec().observation)\n",
    "print('Reward Spec:')\n",
    "print(train_py_env.time_step_spec().reward)\n",
    "print('Action Spec:')\n",
    "print(train_py_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXv5wtPU7WJH"
   },
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1pJNxB87LgT"
   },
   "source": [
    "### Agent\n",
    "\n",
    "* Establish the Critic neural network and Actor neural network\n",
    "* Initailize the SAC discrete agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9K921iKI7N-l"
   },
   "outputs": [],
   "source": [
    "observation_spec = train_env.observation_spec()\n",
    "action_spec = train_env.action_spec()\n",
    "critic_net = critic_network.CriticNetwork(\n",
    "    (observation_spec, action_spec),\n",
    "    observation_fc_layer_params=None,\n",
    "    action_fc_layer_params=None,\n",
    "    joint_fc_layer_params=critic_joint_fc_layer_params)\n",
    "\n",
    "\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    observation_spec,\n",
    "    action_spec,\n",
    "    fc_layer_params=actor_fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8QXYfAvv9YqI"
   },
   "outputs": [],
   "source": [
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "sac_dis_agent = SacDisAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    action_spec,\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=critic_learning_rate),\n",
    "    alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=alpha_learning_rate),\n",
    "    target_update_tau=target_update_tau,\n",
    "    target_update_period=target_update_period,\n",
    "    td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error,\n",
    "    gamma=gamma,\n",
    "    reward_scale_factor=reward_scale_factor,\n",
    "    gradient_clipping=gradient_clipping,\n",
    "    debug_summaries = True,\n",
    "    train_step_counter=global_step)\n",
    "sac_dis_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "KpqPU-cWT3uP",
    "outputId": "5fead847-5d6f-4380-c0a3-e9b826848365"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), observation=BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
       "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
       "      dtype=float32)), action=BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1)), policy_info=(), next_step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)))"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sac_dis_agent.collect_data_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJqvgowsJCyN"
   },
   "source": [
    "### Define replay buffer and trainning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qp4XDcmxI6-A"
   },
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    sac_dis_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity\n",
    ")\n",
    "\n",
    "def collect_training_data():\n",
    "  dynamic_step_driver.DynamicStepDriver(\n",
    "      train_env,\n",
    "      sac_dis_agent.collect_policy,\n",
    "      observers=[replay_buffer.add_batch],\n",
    "      num_steps=initial_collect_steps).run()\n",
    "\n",
    "def train_agent():\n",
    "  dataset = replay_buffer.as_dataset(\n",
    "      sample_batch_size=batch_size,\n",
    "      num_steps=2)\n",
    "\n",
    "  iterator = iter(dataset)\n",
    "\n",
    "  loss = None\n",
    "  for _ in tqdm(range(num_iterations)):\n",
    "    trajectories, _ = next(iterator)\n",
    "    loss = sac_dis_agent.train(experience=trajectories)\n",
    "  \n",
    "  print('\\nTraining loss: ', loss.loss.numpy())\n",
    "  print('\\nTraining loss break down: Critic Loss {}, Actor Loss {}, Alpha Loss {}'.format(loss.extra.critic_loss.numpy(), loss.extra.actor_loss.numpy(), loss.extra.alpha_loss.numpy() ))\n",
    "  return loss.loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eOgoniF_JNFb"
   },
   "source": [
    "### Define evaluation function with previously defined metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kKm5CiJ_JYeQ"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent():\n",
    "  max_score = TFMaxEpisodeScoreMetric() \n",
    "  observers = [max_score]\n",
    "  driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "      train_env, sac_dis_agent.policy, observers, num_episodes=num_eval_episodes)\n",
    "\n",
    "  final_time_step, policy_state = driver.run()\n",
    "\n",
    "  print('Max test score:', max_score.result().numpy())\n",
    "  return max_score.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZvbU6XQQJfdX"
   },
   "source": [
    "### Start training with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZEH3nL4SIPE"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "S-bbM7kFJaiF",
    "outputId": "f16086c9-95ac-4d65-db99-455c1cc39927"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [16:08<00:00, 20.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  1668162200.0\n",
      "\n",
      "Training loss break down: Critic Loss 1668415360.0, Actor Loss -253155.6875, Alpha Loss -27.56578254699707\n",
      "Max test score: 39.949234\n",
      "Step  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [16:00<00:00, 20.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  5921013500000.0\n",
      "\n",
      "Training loss break down: Critic Loss 5921048100864.0, Actor Loss -34847228.0, Alpha Loss -62.44685363769531\n",
      "Max test score: 41.5313\n",
      "Step  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [16:06<00:00, 20.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  185252740000000.0\n",
      "\n",
      "Training loss break down: Critic Loss 185252911775744.0, Actor Loss -160268736.0, Alpha Loss -99.24573516845703\n",
      "Max test score: 58.562542\n",
      "Step  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [16:04<00:00, 20.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  1229401800000000.0\n",
      "\n",
      "Training loss break down: Critic Loss 1229402176225280.0, Actor Loss -422640672.0, Alpha Loss -138.65367126464844\n"
     ]
    }
   ],
   "source": [
    "training_loss = []\n",
    "max_test_score = []\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "  for i in range(10):\n",
    "    print('Step ', i+1)\n",
    "    collect_training_data()\n",
    "    training_loss.append(train_agent())\n",
    "    max_test_score.append(evaluate_agent())\n",
    "  \n",
    "with open('MountCar_training_loss.pkl', 'wb') as f:\n",
    "  pickle.dump(training_loss, f)\n",
    "\n",
    "with open('MountCar_max_test_score.pkl', 'wb') as f2:\n",
    "  pickle.dump(max_test_score, f2)\n",
    "\n",
    "from google.colab import files\n",
    "files.download('MountCar_training_loss.pkl')\n",
    "files.download('MountCar_max_test_score.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mT7VR7qKJpxc"
   },
   "source": [
    "### Performance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BCvEgNECBWk3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "fQsowY50JpbZ",
    "outputId": "25385cf3-eff7-4e17-e212-6990e7027ea5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7d66157470>"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEWCAYAAACDoeeyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2C82yfmi4OH8dAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(1, 21, step = 1), training_loss, c = 'orange', label = 'Training loss')\n",
    "plt.plot(np.arange(1, 21, step = 1), max_test_score, c = 'blue', label = 'Max test score')\n",
    "#plt.axhline(1.715, c = 'gray', linestyle='dashed', label = 'Max possible score')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)\n",
    "plt.title('Training loss and max test score')\n",
    "plt.xticks(np.arange(1, 21))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7ceZaQCbmUi"
   },
   "source": [
    "## Apply SAC discrete on Atari games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KxXLQrnjb0Rx"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpm6rPi7btaQ"
   },
   "outputs": [],
   "source": [
    "env_name = 'Boxing-ram-v0' # @param {type:\"string\"}\n",
    "num_iterations = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000 # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 1000000 # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 256 # @param {type:\"integer\"}\n",
    "\n",
    "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "target_update_tau = 0.005 # @param {type:\"number\"}\n",
    "target_update_period = 1 # @param {type:\"number\"}\n",
    "gamma = 0.99 # @param {type:\"number\"}\n",
    "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
    "gradient_clipping = None # @param\n",
    "\n",
    "actor_fc_layer_params = (256, 256)\n",
    "critic_joint_fc_layer_params = (256, 256)\n",
    "\n",
    "log_interval = 5000 # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 30 # @param {type:\"integer\"}\n",
    "eval_interval = 10 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "5hGgJKunckWA",
    "outputId": "13b668a0-edcc-4d09-c55c-312a52d6dc29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(210, 160, 3), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255)\n",
      "Reward Spec:\n",
      "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=17)\n"
     ]
    }
   ],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(train_py_env.time_step_spec().observation)\n",
    "print('Reward Spec:')\n",
    "print(train_py_env.time_step_spec().reward)\n",
    "print('Action Spec:')\n",
    "print(train_py_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XxbSxkNsdBGZ"
   },
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cF2Oi6lKdCjS"
   },
   "outputs": [],
   "source": [
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "sac_dis_agent = SacDisAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    action_spec,\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=critic_learning_rate),\n",
    "    alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=alpha_learning_rate),\n",
    "    target_update_tau=target_update_tau,\n",
    "    target_update_period=target_update_period,\n",
    "    td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error,\n",
    "    gamma=gamma,\n",
    "    reward_scale_factor=reward_scale_factor,\n",
    "    gradient_clipping=gradient_clipping,\n",
    "    debug_summaries = True,\n",
    "    train_step_counter=global_step)\n",
    "sac_dis_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Scpk-57wdU7X",
    "outputId": "729d4995-130a-45a7-e347-5af3f6706fdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [20:43<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  -0.40372062\n",
      "\n",
      "Training loss break down: Critic Loss 0.00446342071518302, Actor Loss 2.5914273262023926, Alpha Loss -2.9996113777160645\n",
      "Max test score: -56.0\n",
      "Step  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [21:17<00:00,  7.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  -2.6257899\n",
      "\n",
      "Training loss break down: Critic Loss 0.00833912380039692, Actor Loss 3.364788055419922, Alpha Loss -5.998917102813721\n",
      "Max test score: -56.0\n",
      "Step  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [21:02<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  -5.7176065\n",
      "\n",
      "Training loss break down: Critic Loss 0.013141793198883533, Actor Loss 3.269061803817749, Alpha Loss -8.999810218811035\n",
      "Max test score: -56.0\n",
      "Step  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [20:59<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  -8.677102\n",
      "\n",
      "Training loss break down: Critic Loss 0.007756383623927832, Actor Loss 3.3190255165100098, Alpha Loss -12.003884315490723\n",
      "Max test score: -56.0\n",
      "Step  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [21:11<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  -11.835255\n",
      "\n",
      "Training loss break down: Critic Loss 0.003748764283955097, Actor Loss 3.168954849243164, Alpha Loss -15.00795841217041\n",
      "Max test score: -56.0\n",
      "Step  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [20:56<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss:  -15.0280485\n",
      "\n",
      "Training loss break down: Critic Loss 0.002143287332728505, Actor Loss 2.9754538536071777, Alpha Loss -18.005645751953125\n",
      "Max test score: -56.0\n",
      "Step  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 9265/10000 [19:42<01:28,  8.30it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a1cc3b128412>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcollect_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtraining_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmax_test_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-3d7840dfd1f7>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtrajectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msac_dis_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nTraining loss: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tf_agents/agents/tf_agent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, experience, weights)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_functions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m       \u001b[0mloss_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0mloss_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-38ae41e56dec>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, experience, weights)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_actor_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m       \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_numerics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Actor loss is inf or nan.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0mactor_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_actor_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-38ae41e56dec>\u001b[0m in \u001b[0;36mactor_loss\u001b[0;34m(self, time_steps, weights)\u001b[0m\n\u001b[1;32m    296\u001b[0m           common.generate_tensor_summaries('entropy_action',\n\u001b[1;32m    297\u001b[0m                                            \u001b[0maction_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                                            self.train_step_counter)\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m           \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Some distributions do not have an analytic entropy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mgenerate_tensor_summaries\u001b[0;34m(tag, tensor, step)\u001b[0m\n\u001b[1;32m    827\u001b[0m   \"\"\"\n\u001b[1;32m    828\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'histogram'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m     tf.compat.v2.summary.scalar(\n\u001b[1;32m    831\u001b[0m         name='mean', data=tf.reduce_mean(input_tensor=tensor), step=step)\n",
      "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorboard/plugins/histogram/summary_v2.py\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(name, data, step, buckets, description)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_buckets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     return tf.summary.write(\n\u001b[0;32m---> 79\u001b[0;31m         tag=tag, tensor=tensor, step=step, metadata=summary_metadata)\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(tag, tensor, step, metadata, name)\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m   \"\"\"\n\u001b[0;32m--> 615\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write_summary\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, default_name, values)\u001b[0m\n\u001b[1;32m   6296\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6298\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6299\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_eager_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6300\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_symbolic_input_in_eager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/context.py\u001b[0m in \u001b[0;36mcontext_safe\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mcontext_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1577\u001b[0m   \u001b[0;34m\"\"\"Returns current context (or None if one hasn't been initialized).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_loss = []\n",
    "max_test_score = []\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "  for i in range(20):\n",
    "    print('Step ', i+1)\n",
    "    collect_training_data()\n",
    "    training_loss.append(train_agent())\n",
    "    max_test_score.append(evaluate_agent())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lw8jrOZldTqv"
   },
   "source": [
    "### code for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h6xW-kz5dV4I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2sy62cQxdZeS"
   },
   "source": [
    "#### import the lists\n",
    "\n",
    "First get the CartPole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fnxcqXCodeUR"
   },
   "outputs": [],
   "source": [
    "dire = './plot_data'\n",
    "\n",
    "with open(dire + '/' + 'CartPole_training_loss.pkl', 'rb') as f:\n",
    "    ctl = pickle.load(f)\n",
    "with open(dire + '/' + 'CartPole_max_test_score.pkl', 'rb') as f:\n",
    "    cmt = pickle.load(f)\n",
    "with open(dire + '/' + 'DQN-Loss-CrtPole.pkl', 'rb') as f:\n",
    "    dtl = pickle.load(f)\n",
    "with open(dire + '/' + 'DQN-MaxScore-CrtPole.pkl', 'rb') as f:\n",
    "    dmt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hjbPufUkdhEL"
   },
   "source": [
    "Then get the Acrobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "099n5og4di5w"
   },
   "outputs": [],
   "source": [
    "with open(dire + '/' + 'Acrobat_training_loss.pkl', 'rb') as f:\n",
    "    ctl_acro = pickle.load(f)\n",
    "with open(dire + '/' + 'Acrobat_max_test_score.pkl', 'rb') as f:\n",
    "    cmt_acro = pickle.load(f)\n",
    "with open(dire + '/' + 'DQN-Loss-Acrobot.pkl', 'rb') as f:\n",
    "    dtl_acro = pickle.load(f)\n",
    "with open(dire + '/' + 'DQN-MaxScore-Acrobot.pkl', 'rb') as f:\n",
    "    dmt_acro = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tH7WX4vXdk9A"
   },
   "source": [
    "Now get the Mountain Cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDL0vy68dkky"
   },
   "outputs": [],
   "source": [
    "with open(dire + '/' + 'MountCar_training_loss.pkl', 'rb') as f:\n",
    "    ctl_mc = pickle.load(f)\n",
    "with open(dire + '/' + 'MountCar_max_test_score.pkl', 'rb') as f:\n",
    "    cmt_mc = pickle.load(f)\n",
    "with open(dire + '/' + 'DQN-Loss-MountainCar.pkl', 'rb') as f:\n",
    "    dtl_mc = pickle.load(f)\n",
    "with open(dire + '/' + 'DQN-MaxScore-MountainCar.pkl', 'rb') as f:\n",
    "    dmt_mc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NksyjmGMdm2A"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,2, figsize=(18, 16))\n",
    "axs[0,0].plot(np.arange(1, 11, step = 1), normalize(np.array(ctl).reshape(-1,1), axis=0).flatten(), c='orange', label = 'SAC-Discrete')\n",
    "axs[0,0].plot(np.arange(1, 11, step = 1), normalize(np.array(dtl).reshape(-1,1), axis=0).flatten(), c='blue', label = 'DQN')\n",
    "axs[0, 0].set_title('Cart Pole Normalized Training Losses')\n",
    "axs[0, 0].set_xlabel('Epoch')\n",
    "axs[0, 0].set_ylabel('Normalized Loss')\n",
    "axs[0,1].plot(np.arange(1, 11, step = 1), cmt, c='orange', label = 'SAC-Discrete')\n",
    "axs[0,1].plot(np.arange(1, 11, step = 1), dmt, c='blue', label = 'DQN')\n",
    "axs[0, 1].set_title('Cart Pole Max Scores')\n",
    "axs[0, 1].set_xlabel('Epoch')\n",
    "axs[0, 1].set_ylabel('Score')\n",
    "axs[1,0].plot(np.arange(1, 11, step = 1), normalize(np.array(ctl_acro).reshape(-1,1), axis=0).flatten(), c='orange', label = 'SAC-Discrete')\n",
    "axs[1,0].plot(np.arange(1, 11, step = 1), normalize(np.array(dtl_acro).reshape(-1,1), axis=0).flatten(), c='blue', label = 'DQN')\n",
    "axs[1, 0].set_title('Acrobot Normalized Training Losses')\n",
    "axs[1, 0].set_xlabel('Epoch')\n",
    "axs[1, 0].set_ylabel('Normalized Loss')\n",
    "axs[1,1].plot(np.arange(1, 11, step = 1), cmt_acro, c='orange', label = 'SAC-Discrete')\n",
    "axs[1,1].plot(np.arange(1, 11, step = 1), dmt_acro, c='blue', label = 'DQN')\n",
    "axs[1, 1].set_title('Acrobot Normalized Max Scores')\n",
    "axs[1, 1].set_xlabel('Epoch')\n",
    "axs[1, 1].set_ylabel('Score')\n",
    "axs[2,0].plot(np.arange(1, 11, step = 1), normalize(np.array(ctl_mc).reshape(-1,1), axis=0).flatten(), c='orange', label = 'SAC-Discrete')\n",
    "axs[2,0].plot(np.arange(1, 11, step = 1), normalize(np.array(dtl_mc).reshape(-1,1), axis=0).flatten(), c='blue', label = 'DQN')\n",
    "axs[2, 0].set_title('Mountain Car Training Losses')\n",
    "axs[2, 0].set_xlabel('Epoch')\n",
    "axs[2, 0].set_ylabel('Normalized Loss')\n",
    "axs[2,1].plot(np.arange(1, 11, step = 1), cmt_mc, c='orange', label = 'SAC-Discrete')\n",
    "axs[2,1].plot(np.arange(1, 11, step = 1), dmt_mc, c='blue', label = 'DQN')\n",
    "axs[2, 1].set_title('Mountain Car Normalized Max Scores')\n",
    "axs[2, 1].set_xlabel('Epoch')\n",
    "axs[2, 1].set_ylabel('Score')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.legend()\n",
    "\n",
    "fig.savefig('result.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pN0y7ldKyTyE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "GR5242_Reinforcement_learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
